# TinyMLOps
### Um pequeno resumo sobre os desafios operacionais para Adoção generalizada de IA de borda

  O machine laerning é um campo que continua cada vez mais a crescer, e consequentemente também aumenta as suas maneiras de utilização, como por exemplo, a implementação de aplicativos em dispositivo de borda. Esse aspecto de ML pode fornecer uma latência menor, maior robustez, escalabilidade e privacidade em comparação com cenários de implantação em que o modelo é avaliado em infraestrutura em nuvem, entretanto também apresenta seu próprio conjunto de desafios, o obstáculo mais óbvio é o poder de processamento limitado dos dispositivos de borda. E à medida que passamos de um aplicativo centralizado baseado em nuvem para uma implantação descentralizada baseada em borda, novos desafios surgirão que precisam ser enfrentados para dar suporte à adoção bem sucedida do TinyML. 
	
Como foi dito anteriormente, o ML vem tendo uma expansão constante na última década, tivemos técnicas de aprendizado de máquina sendo aplicadas em vários domínios, como finanças, medicina, comércio ou entretenimento, na qual foram desenvolvidos por cientistas de dados. Porém o resultado é um pipeline complicado de vários scripts e estruturas, na qual dificulta muito a implantação do modelo em produção, a um nível que até 85% das iniciativas corporativas de IA estão lutando para ir além dos estágios de teste, ou seja, atualmente, apenas quase 2 projetos de 10 dos projetos de ciência de dados chegam à produção . Para ajudar as organizações a acelerar mais rapidamente nesse importante domínio, é importante entender as operações de ML (MLOps). Em resumo, implantar modelos de ML em escala é muito desafiador, se analisarmos um aplicativo centralizado baseado em nuvem, provavelmente será suficiente ter um único modelo para todos os usuários, simplesmente usando a arquitetura mais recente e melhor para a tarefa em questão, já um aplicativo descentralizado e baseado em borda, o modelo será implantado no dispositivo do usuário final, ou seja, diferentes usuários terão diferentes dispositivos com diferentes recursos computacionais, disponibilidade de armazenamento e conectividade de rede, assim em vez de treinar um único modelo, podemos precisar dar suporte a vários modelos, cada um com seu próprio custo computacional e compensação de precisão, para tornar isso ainda mais complicado, o melhor modelo para um determinado dispositivo pode depender de fatores externos além dos recursos computacionais do dispositivo. Dessa forma, para tentar resolver isso, as organizações precisam construir a cultura e a capacidade de engenharia de ML necessárias. O MLOps visa unificar o desenvolvimento do sistema de ML com as operações do sistema de ML(Ops), defendendo fortemente a automação e o monitoramento em todas as etapas da construção do sistema de ML, desde a integração, teste e liberação até a implantação e gerenciamento de infraestrutura, encurtando o ciclo de vida de desenvolvimento de sistemas e garantir que software de alta qualidade seja continuamente desenvolvido, entregue e mantido em produção. Entretanto esses mecanismos não são tão fáceis de implementar e os desenvolvedores de aplicativos podem não ter tempo ou experiência para faze-lo, assim uma plataforma TinyMLOps que gera automaticamente módulos verificáveis seria, portanto, de grande valor. Já que uma preparação de modelos para TinyML requer etapas extras que um pipeline MLOps padrão não possui, além de que muitas etapas de MLOps padrão precisam ser aumentadas com implementações específicas do TinyML ou até mesmo removidas. É por isso que precisamos do TinyMLOps. Como tudo isso é apenas uma visão geral dos desafios que podem surgir quando um desenvolvedor opta por uma implantação baseada em borda de seu aplicativo de ML em comparação com uma implantação baseada em nuvem, apesar do TinyML ainda ser muito jovem, com a maioria das ferramentas e estruturas ainda em seus estágios iniciais. Espera-se inspirar e orientar o desenvolvimento de plataformas como TinyMLOps, que tornarão o TinyML acessível aos desenvolvedores e escalável para bilhões de dispositivos de ponta.	
